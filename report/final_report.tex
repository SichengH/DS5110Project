%
% DS 5110 (Blue Team) Project Proposal
%
\documentclass[12pt]{article}

%
% Packages
%
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage[utf8]{inputenc}
\usepackage[toc,page]{appendix}

\RequirePackage{graphics}

\usepackage{graphicx}
\graphicspath{ {imgs/} }

%
% Document Settings
%
\setlength{\parskip}{1pc}
\setlength{\parindent}{0pt}
\setlength{\topmargin}{-3pc}
\setlength{\textheight}{9.5in}
\setlength{\oddsidemargin}{0pc}
\setlength{\evensidemargin}{0pc}
\setlength{\textwidth}{6.5in}

\title{\textbf{Flashlight}: Property Assessment Visualization for the City of Boston}
\author{Tyler Brown, Sicheng Hao, Nischal Mahaveer Chand, Sumedh Sankhe}
\date{ }


% START DOCUMENT
\begin{document}

\maketitle

\section*{Summary}

As a new home-buyer, it's easy to find out about your home
but hard to get an understanding of your neighborhood. Flashlight
makes it easier for you to see your potential neighborhood in Boston.
This discrepancy is because current real estate websites emphasize
individual properties rather than individual neighborhoods. Our group
communicates the differences between Boston neighborhoods using an
interactive data visualization called Flashlight.

Our dataset includes Property Assessment history from 2014-2017
\cite{Property49:online} using Boston's Open Data Hub. We enriched the 
property assessment data with coordinates from Open Addresses
\cite{OpenAddr24:online}, and neighborhood boundaries from Zillow
\cite{ZillowNe81:online}. These combined datasets provide unique
insights to new home-buyers in Boston. As open data becomes more
prevalent in cities across the United States, we can scale our insights
and models.

\section*{Methods}

We used methods for collecting, preparing, modeling, and presenting
our data. Each step of the process is detailed here.

\subsection{Data Collection}

We started with Boston's Property Assessment data from 2014-2017
\cite{Property49:online}. This dataset ``[g]ives property, or parcel,
ownership together with value information, which ensures fair assessment
of Boston taxable and non-taxable property of all types and
classifications.''\cite{Property49:online}. We wanted to use this
information because it helps us capture changes in Boston properties
over time. For example, a remodeled property would change it's property
tax assessment value we have this variable available to us.

After starting with the Property Assessment dataset, we brought in
additional datasets to increase the value of our data collection.
Neighborhoods in Boston were not named or geographically demarcated in
the Property Assessment dataset so we brought in Neighborhood Boundaries
from Zillow \cite{ZillowNe81:online} to make this distinction.
Additionally, geographic coordinates for each assessed property's address
were occasionally not coded correctly or included at all for 2017 so we 
had to bring in those values using Open Addresses
\cite{OpenAddr24:online}. Once neighborhood names, boundaries, and missing
coordinates were available, we were able to proceed to data preparation.

\subsection{Data Preparation}

There were a number of steps involved in data preparation.

\subsubsection{Data Audits}

The purpose of a data audit is to answer questions related to data
quantity and quality. We started with our Property Assessment dataset
by checking for the quantity of populated items within each variable.
About 73\% of variables were less than 70\% populated. We were able to
disregard these variables for our modeling purposes. Data quality checks
are not as easily automated but value added.

Analyzing data quality helped us understand data problems up front
such as having a non-unique primary key about $0.3\%$ of the time,
latitude and longitude were missing or corrupted about 35\% of the time,
and the ratio of unique addresses concatenated with coordinates to
unique coordinates were about $3:1$. We also found that the Property
Assessment data did not map to defined neighborhoods in Boston.
Understanding these shortcomings with a data audit allowed our group
to plan remediation steps early in our analysis.


\subsubsection{Geocoding Missing Addresses}

During the data audit we found about 35\% of addresses within the
Property Assessment data were not matched to any coordinate pair. Our
team work to mitigate this deficiency by leveraging the
a ``free and open global address collection'' called the
OpenAddresses \cite{OpenAddr24:online} project. There are several
strategies one can take when Geocoding (mapping address strings to
coordinate pairs) addresses. The simplest would be to use Google Maps
Geocoding API \cite{GettingS89:online}.

The Google Maps Geocoding API has a free usage tier which maxes out at
2,500 requests per day \cite{GettingS89:online}. Our data required
geocoding an order of magnitude more coordinates so this approach was
out of the question. We resolved the issue by creating 'address hashes'
for each address in the OpenAddress and Property Assessment dataset
with missing coordinates. An 'address hash' was a string concatenated
of concatenated values for street number, street name, city, and zip
code. We excluded unit number as a simplifying assumption because we
expected all unit numbers to be at the same property. Once 'address
hash' had been computed for both datasets we performed an inner join.

In some cases, this join generated a many-to-many relationship between
addresses in both datasets due to our exclusion of unit numbers. We
resolved this issue by grouping and taking the first coordinate pair.
Subsequent iterations of our analysis can be improved by adding
robustness to our geocoding procedure.

\subsubsection{Working with GeoJSON and Python}

stuff about leaflet here.

\subsubsection{Data Transformation}



\subsection{Data Modeling}
The purpose of our data modeling was to create a representation of how
prices for properties would change in the next year, all though physical
features of the property play an important role in the value of a property
increase on property value largely depends on the market conditions, surrounding
neighborhoods. Hence to take into account the missing features about market 
it was decided upon to create a simple linear model at first using previous evaluations
to predict for the year 2017. A more important prediction would be to predict how the price
per square feet changes per region, hence the same linear model applied to predict price per sqft. First the price per sqft was calculate for every PID, it was grouped by the region and the mean price per sqft was calculated for that particular region for all years. Next a linear regression model was created with 3 variables and 1 response variable, the variables included the valuations for the year 2014 through 2016 which predicted the evaluations for the year 2017. With an RMSE of 5.45$/sqft and a MAE of 4.477$/sqft the model performed unexcitingly. Considering all the variables were linearly related to the response variable. The above model was applied to predict the price per sqft for the year 2018 taking into considerations the values from 2015 through 2017. Since there was no way to gauge the performance of this model with real data as of now it would be interesting to see the performance.
An interesting problem would be to gauge how housing evaluations are done and how neighborhoods 
end up having such diverse scores even when they are next to each other.
\subsection{Data Presentation}

\section*{Results}

https://sichenghao1992.shinyapps.io/DS5110/

Above is our project on Shiny Server. 

\section*{Discussion}

Let's discuss what we did.

\section*{Statement of Contributions}

Together everyone achieves more.

\begin{itemize}
\item \textbf{Tyler Brown:}
\item \textbf{Sicheng Hao:}
\item \textbf{Nischal Mahaveer Chand:}
\item \textbf{Sumedh Sankhe:}
\end{itemize}

\bibliography{references} 
\bibliographystyle{ieeetr}

\begin{appendices}

https://github.com/sichenghao1992/DS5110Project

Above is our working repository.

\end{appendices}

\end{document}
